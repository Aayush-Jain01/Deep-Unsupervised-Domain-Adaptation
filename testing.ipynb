{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **TODO:** utils.py (mean, std, plot img, covariance), train.py, main.py (call everything), loss.py (accuracy, coral) , plot_loss_accuracy.py\n",
    "- **TODO:** get mean, standard deviation of dataset (each domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total no. of images in amazon dataset: 2817\n"
     ]
    }
   ],
   "source": [
    "# main.py\n",
    "\n",
    "# root_dir = \"datasets/office/amazon/images\"\n",
    "# path = \"datasets/office/amazon\"\n",
    "# name_dataset = \"office\"\n",
    "\n",
    "data_domain = \"amazon\"\n",
    "img_size = 224\n",
    "#\n",
    "# Pytorch ImageFolder fits our dataset structure\n",
    "# dataset = datasets.ImageFolder(root=root_dir, transform=data_transform)\n",
    "image_data_folder = datasets.ImageFolder(root= \"datasets/office/%s/images\" % data_domain)\n",
    "print(\"total no. of images in amazon dataset:\",len(image_data_folder))\n",
    "\n",
    "data_transforms = transforms.Compose([\n",
    "        transforms.Resize((img_size, img_size)),\n",
    "        transforms.RandomSizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225])\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloader.py\n",
    "class OfficeAmazonDataset(Dataset):\n",
    "    \"\"\"Class to create an iterable dataset \n",
    "    of images and corresponding labels \"\"\"\n",
    "        \n",
    "    def __init__(self, image_folder_dataset, transform=None):\n",
    "        super(OfficeAmazonDataset, self).__init__()\n",
    "        self.image_folder_dataset = image_folder_dataset\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_folder_dataset.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # read image, class from folder_dataset given index\n",
    "        img, img_label = image_folder_dataset[idx][0], image_folder_dataset[idx][1]\n",
    "        \n",
    "        # apply transformations (it already returns them as torch tensors)\n",
    "        if self.transform is not None:\n",
    "            self.transform(img)\n",
    "        \n",
    "        img_label_pair = {\"image\": img,\n",
    "                         \"class\": img_label}\n",
    "        \n",
    "        return img_label_pair\n",
    "\n",
    "def get_dataloader(dataset, batch_size):\n",
    "        \n",
    "    def get_subset(indices, start, end):\n",
    "        return indices[start:start + end]\n",
    "    \n",
    "    # Split train/val data ratios\n",
    "    TRAIN_RATIO, VALIDATION_RATIO = 0.7, 0.3\n",
    "    train_set_size = int(len(dataset) * TRAIN_RATIO)\n",
    "    validation_set_size = int(len(dataset) * VALIDATION_RATIO)\n",
    "\n",
    "    # Generate random indices for train and val sets\n",
    "    indices = torch.randperm(len(dataset))\n",
    "    train_indices = get_subset(indices, 0, train_set_size)\n",
    "    validation_indices = get_subset(indices, train_set_size, validation_set_size)\n",
    "    # test_indices = get_subset(indices, train_count + validation_count, len(dataset))\n",
    "\n",
    "    # Create sampler objects \n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(validation_indices)\n",
    "\n",
    "    # Create data loaders for data\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                              sampler=train_sampler, num_workers=4)\n",
    "\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, \n",
    "                            sampler=val_sampler, num_workers=4)\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_dataset = OfficeAmazonDataset(image_folder_dataset=image_data_folder, transform=data_transforms)\n",
    "train_loader, val_loader = get_dataloader(amazon_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python36] *",
   "language": "python",
   "name": "conda-env-Python36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
